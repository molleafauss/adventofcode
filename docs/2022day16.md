## Using Advent of code 2022 day 16 to compare languages
For fun, I decided to port the solution to [Advent of code 2022, Day 16](https://adventofcode.com/2022/day/16) 
to few languages I know. Just to compare the results and see if I would find some surprises. 
Surprises happened.

The problem is complex, it's a recursive search algorithm that requires some memoization to 
reduce the time to completion, overall it definitely taxes memory and cpu in terms of sheer 
number of function calls and data memorized.

The four languages I ported the solution are:
* python (a language I know well enough, and it was the original I used for the challenge)
* Rust (I language I learned recently, and that I ported first the python version to)
* Java (my main language, the one I've worked the most in these past 25 years)
* Go (a recently learned language that I need for my new workplace)

> IMPORTANT CAVEATS: The algorithm used to solve the problem is for sure awful and can be 
> improved and optimised in many ways. The results below are quite interesting, though.

So what are the results? I tried to measure not only the time it took to solve the challenge, 
but tried to check cpu and memory used, just to make some comparison.

> machine used: Windows 11 - Intel Core i7-10875H CPU @ 2.30GHz - 32GB RAM

| LANGUAGE | SOLUTION TIME | CPU USAGE           | MEMORY PEAK        |
|----------|---------------|---------------------|--------------------|
| Python   | 450.8sec      | avg & peak 100%     | 16GB               | 
| Rust     | 583.9sec (*)  | avg & peak 100%     | 10GB (peaks at 16) | 
| Java     | 209.5sec      | avg 300%, peak 350% | 8.5GB              | 
| Go       | 169.6sec      | avg 100%, peak 500% | 10.2GB             |
(*) Rust takes ~30s to output the final result, due to running Drop on all allocated entities in
    the cache. Funny enough this delay happens only on windows.

Language versions:
* Python 3.12.8
* go version go1.23.3 windows/amd64
* rustc 1.83.0 (90b35a623 2024-11-26) stable-x86_64-pc-windows-msvc
* OpenJDK 64-Bit Server VM Corretto-21.0.2.13.1 (build 21.0.2+13-LTS, mixed mode, sharing)

But how? Rust should be the fastest! ... Hold yer horses, let's try some hypothesis.

* in Java and Go CPU peaks over 100%; the procedure is single-threaded, and there's no 
  concurrency involved. This suggests both languages' GC is using the other CPUs while the main 
  thread runs the program, minimizing pauses.
  * In fact, on java, when using the `-XX:+UseSerialGC` switch that uses the 
    serial/single-threaded GC, run time jump to 333secs and the CPU usage in fact flattens to 
    100%. Still, is faster than Rust :thinking_face: 
  * python has also a GC, but doesn't use multiple core for memory management.
  * Rust is not a GC language, so the main thread does all the allocation/deallocation 
    while the program runs; with over 100M calls happening, this might be the reason for the 
    slowness. The procedure was also copied from python and may not play nice with the strict 
    safety that Rust enforces.
* Memory: Java seems the most effective here, Go is close enough to Java, and Rust is similar, 
  with a small peak at 16GB, probably when the HashMap used for cache was being resized. 

## Any optimization we should o?
So the main problem here is the size of the cache: we're talking ~30M entries and 80M+ cache 
hits over 110M+ calls of the procedure.
- the map key includes two strings (human and elephant position), two ints (human and elephant 
  time) and a bit mask describing which valves are open.
  - positions can be an index in the array of valves with flow, and a single byte each could be 
    enough, all inputs have less than 256 valves; even better, we only need to go to each valve 
    with flow, which are maximum 15 in my puzzle, so half a byte would be more than enough; time 
    also never passes 30 so one byte each to represent human and elephant time could be enough. 
    the open valves bitmak requires one bit for every valve, we have 16 bits here that are 
    enough. Summing up: 1 byte for the positions, 2 bytes for the time, 16 bits/2 bytes for the 
    open valve mask -> we still need a 64bit int (maybe there's some trick to compress in 32 bits, 
    but the cache key is not the major memory hog).
- the content of the cache should still contain a reasonable amount of data, however; instead of a 
  list of strings, we could have a plain array of ints (or bytes) each representing the position of 
  valves in the global array, and then optimize the other values like for the key
- rust does also a lot of clone/copies to avoid ownership issues; moving structs to primitives may 
  speed up things there.
- The solution also calculates the path walked; the puzzle did not strictly require it as part 
  of the solution. It could be dropped, and we could just store the total flow (for which an 
  int/short would be enough), but it's nice to have.

We'll be using Java first for the optimization and then try to port them to the other languages.

### First optimization: using primitives.
Moving out the strings from the Java program improved execution time to around 75sec, about 
30% time of the initial implementation.

You can check the commit here [b19c589084e2f945064c046c968b909fc80e0bb7].

To understand which part of the program was more troublesome, I added some "handcrafted" 
profiling; it's not surprising that the cache operations are those consuming the most 
of the execution time.

```
INFO | path.merge: calls: 81105106, avg: 76,18ns, max: 203356600ns, min: 0ns, 4,81% of total
INFO | path.next_human: calls: 213032617, avg: 50,15ns, max: 230496600ns, min: 0ns, 8,31% of total
INFO | path.next_elephant: calls: 213032617, avg: 48,90ns, max: 192834800ns, min: 0ns, 8,10% of total
INFO | cache.get: calls: 111150141, avg: 559,54ns, max: 1813007300ns, min: 0ns, 48,38% of total
INFO | cache.put: calls: 30045035, avg: 412,22ns, max: 4316748900ns, min: 0ns, 9,63% of total
```

Half of the overall execution time is spent in getting the item from the cache. This 
cache uses the default hashmap, meaning that for every get or put we hash the key object (which 
is a composite value), find its bucket, and then possibly walk through a tree.

We're already using a `long` for the cache key, so hashing should probably be nearly immediate, 
barring the time for boxing into a `Long`.

We probably want a specialized structure for caching.
